'''base on tunemodel, I think title contains info of gender----correlation'''
'''SCALES FARE AND AGE'''
'''categorical vars should we turn it to #s'''

import csv
import numpy as np
import time
from sklearn import cross_validation

 


def readfile(path, cols_index):

    with open(path, 'rb') as infile:
        reader = csv.reader(infile)
        header = reader.next()
        x = []
        for line in reader:
            cols = [line[t] for t in cols_index]
            x.append(cols)
    x = np.array(x)
    return x 


def prtdata(x, num_obs):

    print 'Sample of data: . The size of data:'
    print x[0:num_obs, 0::]
    print x.shape
    print ''


def checkmiss(x):

    cols = x.shape[1]
    num_missing = []
    perc_missing = []
    for j in range(cols):
        num_missing.append(x[x[::,j]=='',0::].shape[0])
        perc_missing.append(num_missing[j]/cols)
    
    print '# of missing values of training.csv'
    print num_missing
    print ''
    print 'percentage of missing values of training.csv'
    print perc_missing 
    print ''
    return num_missing, perc_missing





def _mean_col(x, col):

    meancol = np.round(np.mean(x[x[0::,col]!='',col].astype(np.float)),2)
    return meancol



def _median_col(x, col):

    mediancol = np.median(x[x[0::,col]!='',col].astype(np.float))
    return mediancol



def impu(x, col, c):
    x[x[0::,col]=='',col]= c
    return x



 
 
def gender(x,col):
    x[x[0::,col]=='male',col] = 1
    x[x[0::,col]=='female',col]=0
    return x

 

 

 
def preparedata(x, size):

    y = x[:,0].astype(np.int)
    x = x[:,1:]
    x_train, x_cvtest, y_train, y_cvtest = cross_validation.train_test_split(x,y,test_size = size, random_state = 0)
    return x_train, x_cvtest, y_train, y_cvtest



    




path_1 = 'C:/Users/qing/Desktop/train.csv'
x = readfile(path_1, [1,2,4, 5, 6,7, 9,11])
## columns: survive 1, pclass 2, gender 4, age 5, sibsp 6, parch 7, fare 9, embark 11

prtdata(x, 3)
checkmiss(x)
_mean_x = _mean_col(x, 3)
_median_x = _median_col(x, 3) 
print 'mean of x'
print _mean_x
'29.7'
x = impu(x, 3, _mean_x)
print x[:5,::]

 


#impute embark, there are 2 missing values
print '#impute embark, there are 2 missing values'
print x[x[0::,-1]=='',0::]
x[x[0::,-1]=='',-1] = 'S'
'''#impute embark, there are 2 missing values
[['1' '1' 'female' '38' '0' '0' '80' '']
 ['1' '1' 'female' '62' '0' '0' '80' '']]'''

checkmiss(x)



nm = readfile(path_1, [3])
m = nm.shape[0]
nm = np.reshape(nm, m)
last_titfir_nm = [t.split(',') for t in nm]
titfir_nm = [t[1] for t in last_titfir_nm]
tit_fir_nm = [t.split() for t in titfir_nm]
title = [t[0] for t in tit_fir_nm]
print title[:5]

## lable title
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit(title)
title = le.transform(title)
print ''
print 'title in training.csv have now:'
title_x = list(le.classes_)
print title_x  
print ''
print 'lable title now:'
print x[0:5, 0::]
 


## combine title into x as the last column
title = np.array(title)
title = np.reshape(title,(m,1))
print x.shape
print title.shape
x = np.hstack((x,title))
print ''
print 'combining title as the last column now:'
print x[0:5, 0::] 


## lable embark
from sklearn import preprocessing
lee = preprocessing.LabelEncoder()
lee.fit(x[0::,-2])
x[0::,-2] = lee.transform(x[0::,-2])
print ''
print 'lable embark now:'
print x[0:5, 0::]



## lable gender
x = gender(x,2)
print ''
print 'lable gender now:'
print x[0:5, 0::]
 


 

 


x_train, x_cvtest, y_train, y_cvtest = preparedata(x, 0.3)
print 'training sample, ready for use'
print x_train[:5,0::]
print 'cvtest sample, ready for use'
print x_cvtest[:5,0::]
 
 
 


 
 


 


 



 

 



import sklearn
from sklearn.neighbors import KNeighborsClassifier
def knnclf(x_train, y_train, x_cvtest, y_cvtest):
    
    clf = KNeighborsClassifier(n_neighbors = 2)
    clf.fit(x_train,y_train)

    print 'accuracy in training set: %f' %clf.score(x_train, y_train)
    print 'accuracy in cvtest set: %f' %clf.score(x_cvtest, y_cvtest)
    return clf




'''from sklearn.ensemble import RandomForestClassifier
def rfclf(x_train, y_train, x_cvtest, y_cvtest):
    
    clf = RandomForestClassifier(n_estimators = 100)
    clf.fit(x_train,y_train)

    print 'accuracy in training set: %f' %clf.score(x_train, y_train)
    print 'accuracy in cvtest set: %f' %clf.score(x_cvtest, y_cvtest)
    print 'feature_importances_'
    print clf.feature_importances_
    return clf'''




 



from sklearn.svm import SVC
def svc_rbf_clf(x_train, y_train, x_cvtest, y_cvtest):
    
    clf = SVC(kernel = 'rbf')
    clf.fit(x_train,y_train)

    print 'accuracy in training set: %f' %clf.score(x_train, y_train)
    print 'accuracy in cvtest set: %f' %clf.score(x_cvtest, y_cvtest)
    return clf


from sklearn.svm import LinearSVC 
def linearsvc_clf(x_train, y_train, x_cvtest, y_cvtest):
    
    clf = sklearn.svm.LinearSVC()
    clf.fit(x_train,y_train)

    print 'accuracy in training set: %f' %clf.score(x_train, y_train)
    print 'accuracy in cvtest set: %f' %clf.score(x_cvtest, y_cvtest)
    return clf



from sklearn.linear_model import LogisticRegression
def logisclf_l1(x_train, y_train, x_cvtest, y_cvtest):
    
    clf = LogisticRegression(penalty = 'l1', random_state = None)
    clf.fit(x_train,y_train)

    print 'accuracy in training set: %f' %clf.score(x_train, y_train)
    print 'accuracy in cvtest set: %f' %clf.score(x_cvtest, y_cvtest)
    return clf


from sklearn.linear_model import LogisticRegression
def logisclf_l2(x_train, y_train, x_cvtest, y_cvtest):
    
    clf = LogisticRegression(penalty = 'l2', random_state = None)
    clf.fit(x_train,y_train)

    print 'accuracy in training set: %f' %clf.score(x_train, y_train)
    print 'accuracy in cvtest set: %f' %clf.score(x_cvtest, y_cvtest)
    return clf


 

print 'knn'
knnclf(x_train, y_train, x_cvtest, y_cvtest)
print ''

'''print 'random forest'
rfclf(x_train, y_train, x_cvtest, y_cvtest)
print ''
'''

 


print 'svc'
svc_rbf_clf(x_train, y_train, x_cvtest, y_cvtest)
print ''

'''print 'linearsvc'
linearsvc_clf(x_train, y_train, x_cvtest, y_cvtest)
print ''


print 'logistic with l1'
logisclf_l1(x_train, y_train, x_cvtest, y_cvtest)
print ''

print 'logistic with l2'
logisclf_l2(x_train, y_train, x_cvtest, y_cvtest)'''

##rf 

 


 




print ''
print ''
print ''
print ''
print ''
print ''











### prediction 

print 'now predicting:'
print ''

path_2 = 'C:/Users/qing/Desktop/test.csv'
tstx = readfile(path_2,[1,3, 4, 5,6, 8,10])
# features: pclass 1, gender3, age 4, sibsp 5, parch 6, fare8, embarked 10
prtdata(tstx, 3)



checkmiss(tstx)
_mean_tstx = _mean_col(tstx, 2)
print 'mean of age, ready for imputation'
print _mean_tstx
_median_tstx = _median_col(tstx, 2)
tstx = impu(tstx, 2, _mean_tstx)
print ''
print 'impute missing ages:'
print tstx[0:5,0::]
 

 

_mean_fare = _mean_col(tstx, -2)
print 'mean of fare in test set'
print _mean_fare
'35.63'
tstx = impu(tstx, -2, _mean_fare)
checkmiss(tstx)



## lable embark 
tstx[0::,-1] = lee.transform(tstx[0::,-1])
print ''
print 'lable embark now:'
print tstx[0:5, 0::]


## lable gender
print ''
print 'label gender now:'
tstx = gender(tstx, 1)
print tstx[0:5, 0::]










## read title
nmtst = readfile(path_2, [2])
mtst = nmtst.shape[0]
nmtst = np.reshape(nmtst, mtst)
last_titfir_nmtst = [t.split(',') for t in nmtst]
titfir_nmtst = [t[1] for t in last_titfir_nmtst]
tit_fir_nmtst = [t.split() for t in titfir_nmtst]
titletst = [t[0] for t in tit_fir_nmtst]
print titletst[:5]
 

 
 

## combine titletst into tstx as the last column
titletst = np.array(titletst)
titletst = np.reshape(titletst,(mtst,1))
print tstx.shape
print titletst.shape
tstx = np.hstack((tstx,titletst))
print tstx[0:5, 0::]



print ''
print 'there are %i of different titles in the training.csv' %len(title_x)
print title_x
print ''
 

## lable title

def tsttitle_lable(tx, col):

    for i in range(len(title_x)):
        tx[tx[0::,col]==title_x[i], col] = i
    return tx

 
tstx = tsttitle_lable(tstx, -1)  
print ''
print 'lable title now:'
print tstx[:5,0::]
 


print 'check title in the test'
print tstx[0::,-1]
print ''
print ''
print 'label Dona as 2'
tstx[tstx[0::,-1]=='Dona.', -1] = 2
print ''
print ''
print 'check title in the test'
print tstx[0::,-1]



 

print ''
print 'compare training sample and test sample:'
print x_train[0:5, 0::]
print ''
print tstx[0:5, 0::]
print ''
print ''








from sklearn.ensemble import RandomForestClassifier
def predit(x_train, y_train, x_cvtest, y_cvtest, tstx):
    
    clf = RandomForestClassifier(n_estimators = 100)
    clf.fit(x_train,y_train)
    pred = clf.predict(tstx)
    
    print 'accuracy in training set: %f' %clf.score(x_train, y_train)
    print 'accuracy in cvtest set: %f' %clf.score(x_cvtest, y_cvtest)
    print 'feature_importances_'
    print clf.feature_importances_
    return pred



 
 
print 'Here are the predictions with Random Forest'
pred = predit(x_train, y_train, x_cvtest, y_cvtest, tstx)
print pred

 
pred = list(pred)
 

## for submission


with open(path_2, 'rb') as infile:
        reader = csv.reader(infile)
        header = reader.next()
        ids = []
        for line in reader:
            PassengerId,Pclass,	Name,	Sex,	Age,	SibSp,	Parch,	Ticket,	Fare,	Cabin,	Embarked = line
            ids.append(PassengerId)

            

 

import csv
path_3 = 'C:/Users/qing/Desktop/submission.csv'
with open(path_3,'wb') as outfile:
    writer = csv.writer(outfile, delimiter = ',')
    writer.writerow(['PassengerID','Survived'])
    writer.writerows(zip(ids,pred))

























'''>>> 
Sample of data: . The size of data:
[['0' '3' 'male' '22' '1' '0' '7.25' 'S']
 ['1' '1' 'female' '38' '1' '0' '71.2833' 'C']
 ['1' '3' 'female' '26' '0' '0' '7.925' 'S']]
(891, 8)

# of missing values of training.csv
[0, 0, 0, 177, 0, 0, 0, 2]

percentage of missing values of training.csv
[0, 0, 0, 22, 0, 0, 0, 0]

mean of x
29.7
[['0' '3' 'male' '22' '1' '0' '7.25' 'S']
 ['1' '1' 'female' '38' '1' '0' '71.2833' 'C']
 ['1' '3' 'female' '26' '0' '0' '7.925' 'S']
 ['1' '1' 'female' '35' '1' '0' '53.1' 'S']
 ['0' '3' 'male' '35' '0' '0' '8.05' 'S']]
#impute embark, there are 2 missing values
[['1' '1' 'female' '38' '0' '0' '80' '']
 ['1' '1' 'female' '62' '0' '0' '80' '']]
# of missing values of training.csv
[0, 0, 0, 0, 0, 0, 0, 0]

percentage of missing values of training.csv
[0, 0, 0, 0, 0, 0, 0, 0]

['Mr.', 'Mrs.', 'Miss.', 'Mrs.', 'Mr.']

title in training.csv have now:
['Capt.', 'Col.', 'Don.', 'Dr.', 'Jonkheer.', 'Lady.', 'Major.', 'Master.', 'Miss.', 'Mlle.', 'Mme.', 'Mr.', 'Mrs.', 'Ms.', 'Rev.', 'Sir.', 'the']

lable title now:
[['0' '3' 'male' '22' '1' '0' '7.25' 'S']
 ['1' '1' 'female' '38' '1' '0' '71.2833' 'C']
 ['1' '3' 'female' '26' '0' '0' '7.925' 'S']
 ['1' '1' 'female' '35' '1' '0' '53.1' 'S']
 ['0' '3' 'male' '35' '0' '0' '8.05' 'S']]
(891, 8)
(891, 1)

combining title as the last column now:
[['0' '3' 'male' '22' '1' '0' '7.25' 'S' '11']
 ['1' '1' 'female' '38' '1' '0' '71.2833' 'C' '12']
 ['1' '3' 'female' '26' '0' '0' '7.925' 'S' '8']
 ['1' '1' 'female' '35' '1' '0' '53.1' 'S' '12']
 ['0' '3' 'male' '35' '0' '0' '8.05' 'S' '11']]

lable embark now:
[['0' '3' 'male' '22' '1' '0' '7.25' '2' '11']
 ['1' '1' 'female' '38' '1' '0' '71.2833' '0' '12']
 ['1' '3' 'female' '26' '0' '0' '7.925' '2' '8']
 ['1' '1' 'female' '35' '1' '0' '53.1' '2' '12']
 ['0' '3' 'male' '35' '0' '0' '8.05' '2' '11']]

lable gender now:
[['0' '3' '1' '22' '1' '0' '7.25' '2' '11']
 ['1' '1' '0' '38' '1' '0' '71.2833' '0' '12']
 ['1' '3' '0' '26' '0' '0' '7.925' '2' '8']
 ['1' '1' '0' '35' '1' '0' '53.1' '2' '12']
 ['0' '3' '1' '35' '0' '0' '8.05' '2' '11']]
training sample, ready for use
[['1' '1' '51' '0' '0' '26.55' '2' '11']
 ['1' '0' '49' '1' '0' '76.7292' '0' '12']
 ['3' '1' '1' '5' '2' '46.9' '2' '7']
 ['1' '1' '54' '0' '1' '77.2875' '2' '11']
 ['3' '0' '29.7' '1' '0' '14.4583' '0' '12']]
cvtest sample, ready for use
[['3' '1' '29.7' '0' '0' '14.4583' '0' '11']
 ['3' '1' '29.7' '0' '0' '7.55' '2' '11']
 ['3' '1' '7' '4' '1' '29.125' '1' '7']
 ['1' '0' '29.7' '1' '0' '146.5208' '0' '12']
 ['3' '0' '29' '0' '2' '15.2458' '0' '12']]
knn
accuracy in training set: 0.834671
accuracy in cvtest set: 0.701493

svc
accuracy in training set: 0.905297
accuracy in cvtest set: 0.716418







now predicting:

Sample of data: . The size of data:
[['3' 'male' '34.5' '0' '0' '7.8292' 'Q']
 ['3' 'female' '47' '1' '0' '7' 'S']
 ['2' 'male' '62' '0' '0' '9.6875' 'Q']]
(418, 7)

# of missing values of training.csv
[0, 0, 86, 0, 0, 1, 0]

percentage of missing values of training.csv
[0, 0, 12, 0, 0, 0, 0]

mean of age, ready for imputation
30.27

impute missing ages:
[['3' 'male' '34.5' '0' '0' '7.8292' 'Q']
 ['3' 'female' '47' '1' '0' '7' 'S']
 ['2' 'male' '62' '0' '0' '9.6875' 'Q']
 ['3' 'male' '27' '0' '0' '8.6625' 'S']
 ['3' 'female' '22' '1' '1' '12.2875' 'S']]
mean of fare in test set
35.63
# of missing values of training.csv
[0, 0, 0, 0, 0, 0, 0]

percentage of missing values of training.csv
[0, 0, 0, 0, 0, 0, 0]


lable embark now:
[['3' 'male' '34.5' '0' '0' '7.8292' '1']
 ['3' 'female' '47' '1' '0' '7' '2']
 ['2' 'male' '62' '0' '0' '9.6875' '1']
 ['3' 'male' '27' '0' '0' '8.6625' '2']
 ['3' 'female' '22' '1' '1' '12.2875' '2']]

label gender now:
[['3' '1' '34.5' '0' '0' '7.8292' '1']
 ['3' '0' '47' '1' '0' '7' '2']
 ['2' '1' '62' '0' '0' '9.6875' '1']
 ['3' '1' '27' '0' '0' '8.6625' '2']
 ['3' '0' '22' '1' '1' '12.2875' '2']]
['Mr.', 'Mrs.', 'Mr.', 'Mr.', 'Mrs.']
(418, 7)
(418, 1)
[['3' '1' '34.5' '0' '0' '7.8292' '1' 'Mr.']
 ['3' '0' '47' '1' '0' '7' '2' 'Mrs.']
 ['2' '1' '62' '0' '0' '9.6875' '1' 'Mr.']
 ['3' '1' '27' '0' '0' '8.6625' '2' 'Mr.']
 ['3' '0' '22' '1' '1' '12.2875' '2' 'Mrs.']]

there are 17 of different titles in the training.csv
['Capt.', 'Col.', 'Don.', 'Dr.', 'Jonkheer.', 'Lady.', 'Major.', 'Master.', 'Miss.', 'Mlle.', 'Mme.', 'Mr.', 'Mrs.', 'Ms.', 'Rev.', 'Sir.', 'the']


lable title now:
[['3' '1' '34.5' '0' '0' '7.8292' '1' '11']
 ['3' '0' '47' '1' '0' '7' '2' '12']
 ['2' '1' '62' '0' '0' '9.6875' '1' '11']
 ['3' '1' '27' '0' '0' '8.6625' '2' '11']
 ['3' '0' '22' '1' '1' '12.2875' '2' '12']]
check title in the test
['11' '12' '11' '11' '12' '11' '8' '11' '12' '11' '11' '11' '12' '11' '12'
 '12' '11' '11' '8' '12' '11' '7' '12' '11' '12' '11' '8' '11' '11' '11'
 '11' '11' '12' '12' '11' '11' '8' '8' '11' '11' '11' '11' '11' '12' '12'
 '11' '11' '11' '12' '12' '11' '11' '8' '8' '11' '7' '11' '11' '11' '8'
 '11' '11' '11' '8' '7' '12' '8' '11' '11' '12' '8' '11' '8' '11' '8' '11'
 '11' '12' '11' '8' '7' '11' '11' '11' '11' '11' '8' '8' '13' '7' '12' '11'
 '12' '11' '11' '11' '12' '11' '8' '11' '12' '11' '11' '11' '12' '11' '11'
 '11' '11' '11' '11' '8' '8' '8' '12' '11' '11' '8' '11' '12' '8' '11' '12'
 '11' '11' '8' '11' '8' '11' '11' '11' '1' '12' '11' '11' '11' '11' '11'
 '8' '11' '8' '8' '11' '11' '11' '11' '11' '11' '11' '14' '12' '11' '11'
 '12' '7' '11' '8' '8' '11' '12' '8' '7' '8' '11' '14' '12' '11' '11' '12'
 '8' '11' '11' '11' '11' '11' '8' '8' '11' '12' '12' '11' '11' '12' '11'
 '12' '11' '8' '11' '8' '11' '11' '11' '7' '11' '7' '11' '7' '8' '11' '12'
 '8' '7' '1' '8' '11' '11' '8' '11' '8' '11' '11' '11' '11' '12' '8' '11'
 '8' '11' '12' '11' '8' '11' '12' '11' '12' '12' '11' '8' '11' '11' '11'
 '8' '11' '11' '11' '11' '11' '11' '8' '12' '12' '12' '11' '11' '7' '11'
 '12' '11' '12' '12' '8' '11' '11' '11' '11' '11' '11' '11' '8' '11' '11'
 '11' '12' '8' '11' '11' '11' '11' '8' '11' '11' '11' '12' '8' '11' '8'
 '11' '11' '11' '11' '8' '7' '8' '8' '8' '11' '11' '11' '11' '11' '11' '8'
 '11' '3' '11' '11' '8' '11' '11' '11' '11' '11' '11' '11' '8' '12' '11'
 '7' '11' '12' '11' '11' '11' '8' '12' '8' '11' '11' '11' '11' '11' '11'
 '11' '11' '8' '11' '8' '11' '11' '11' '12' '11' '11' '12' '11' '11' '11'
 '11' '11' '7' '11' '11' '11' '12' '7' '8' '11' '12' '11' '8' '12' '11'
 '11' '11' '8' '11' '12' '11' '11' '12' '7' '12' '12' '11' '12' '12' '11'
 '8' '12' '11' '11' '8' '11' '11' '12' '8' '8' '11' '11' '7' '11' '11' '12'
 '12' '11' '8' '11' '11' '11' '7' '11' '12' '7' '11' '11' '12' '11' '12'
 '11' '11' '8' '11' '8' '11' '11' '11' '11' '11' '8' '8' '8' '12' '8' '11'
 'Dona.' '11' '11' '7']


label Dona as 2


check title in the test
['11' '12' '11' '11' '12' '11' '8' '11' '12' '11' '11' '11' '12' '11' '12'
 '12' '11' '11' '8' '12' '11' '7' '12' '11' '12' '11' '8' '11' '11' '11'
 '11' '11' '12' '12' '11' '11' '8' '8' '11' '11' '11' '11' '11' '12' '12'
 '11' '11' '11' '12' '12' '11' '11' '8' '8' '11' '7' '11' '11' '11' '8'
 '11' '11' '11' '8' '7' '12' '8' '11' '11' '12' '8' '11' '8' '11' '8' '11'
 '11' '12' '11' '8' '7' '11' '11' '11' '11' '11' '8' '8' '13' '7' '12' '11'
 '12' '11' '11' '11' '12' '11' '8' '11' '12' '11' '11' '11' '12' '11' '11'
 '11' '11' '11' '11' '8' '8' '8' '12' '11' '11' '8' '11' '12' '8' '11' '12'
 '11' '11' '8' '11' '8' '11' '11' '11' '1' '12' '11' '11' '11' '11' '11'
 '8' '11' '8' '8' '11' '11' '11' '11' '11' '11' '11' '14' '12' '11' '11'
 '12' '7' '11' '8' '8' '11' '12' '8' '7' '8' '11' '14' '12' '11' '11' '12'
 '8' '11' '11' '11' '11' '11' '8' '8' '11' '12' '12' '11' '11' '12' '11'
 '12' '11' '8' '11' '8' '11' '11' '11' '7' '11' '7' '11' '7' '8' '11' '12'
 '8' '7' '1' '8' '11' '11' '8' '11' '8' '11' '11' '11' '11' '12' '8' '11'
 '8' '11' '12' '11' '8' '11' '12' '11' '12' '12' '11' '8' '11' '11' '11'
 '8' '11' '11' '11' '11' '11' '11' '8' '12' '12' '12' '11' '11' '7' '11'
 '12' '11' '12' '12' '8' '11' '11' '11' '11' '11' '11' '11' '8' '11' '11'
 '11' '12' '8' '11' '11' '11' '11' '8' '11' '11' '11' '12' '8' '11' '8'
 '11' '11' '11' '11' '8' '7' '8' '8' '8' '11' '11' '11' '11' '11' '11' '8'
 '11' '3' '11' '11' '8' '11' '11' '11' '11' '11' '11' '11' '8' '12' '11'
 '7' '11' '12' '11' '11' '11' '8' '12' '8' '11' '11' '11' '11' '11' '11'
 '11' '11' '8' '11' '8' '11' '11' '11' '12' '11' '11' '12' '11' '11' '11'
 '11' '11' '7' '11' '11' '11' '12' '7' '8' '11' '12' '11' '8' '12' '11'
 '11' '11' '8' '11' '12' '11' '11' '12' '7' '12' '12' '11' '12' '12' '11'
 '8' '12' '11' '11' '8' '11' '11' '12' '8' '8' '11' '11' '7' '11' '11' '12'
 '12' '11' '8' '11' '11' '11' '7' '11' '12' '7' '11' '11' '12' '11' '12'
 '11' '11' '8' '11' '8' '11' '11' '11' '11' '11' '8' '8' '8' '12' '8' '11'
 '2' '11' '11' '7']

compare training sample and test sample:
[['1' '1' '51' '0' '0' '26.55' '2' '11']
 ['1' '0' '49' '1' '0' '76.7292' '0' '12']
 ['3' '1' '1' '5' '2' '46.9' '2' '7']
 ['1' '1' '54' '0' '1' '77.2875' '2' '11']
 ['3' '0' '29.7' '1' '0' '14.4583' '0' '12']]

[['3' '1' '34.5' '0' '0' '7.8292' '1' '11']
 ['3' '0' '47' '1' '0' '7' '2' '12']
 ['2' '1' '62' '0' '0' '9.6875' '1' '11']
 ['3' '1' '27' '0' '0' '8.6625' '2' '11']
 ['3' '0' '22' '1' '1' '12.2875' '2' '12']]


Here are the predictions with Random Forest
accuracy in training set: 0.982343
accuracy in cvtest set: 0.809701
feature_importances_
[ 0.07808802  0.18405356  0.24498859  0.05997972  0.03526993  0.23397672
  0.03595278  0.12769068]
[0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0
 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0
 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0
 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0
 1 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1
 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0
 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1
 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0
 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0
 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1
 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0
 0 0 1 0 1 0 0 1 0 0 1]
'''



 
